# Data
data:
  instruct_data: "/home/rileywebb/datasets/combined_dataset.jsonl"  # Path to the full combined dataset
  data: ""  # Optional pretraining data
  eval_instruct_data: "/home/rileywebb/datasets/eval_dataset.jsonl"  # Optional evaluation data

# Model
model_id_or_path: "/home/rileywebb/mistral_models/7B/instruct"  # Path to Mistral 7B Instruct model
lora:
  rank: 8  # Balanced LoRA rank for capacity and memory
  alpha: 32  # LoRA scaling factor
  dropout: 0.1  # LoRA dropout to mitigate overfitting

# Optimization
seq_len: 32768  # Required context length
batch_size: 1  # Due to memory constraints
gradient_accumulation_steps: 32  # Simulate an effective batch size of 32
max_steps: 2500  # Adjust based on dataset size for adequate training
optim:
  lr: 1e-4  # Learning rate suitable for fine-tuning with small batches
  weight_decay: 0.0  # No weight decay for fine-tuning
  eps: 1e-6  # Epsilon for numerical stability
  lr_scheduler_type: "cosine"  # Cosine decay learning rate scheduler
  warmup_steps: 100  # Warm-up steps to stabilize training

# Other
seed: 42  # For reproducibility
log_freq: 5  # Frequent logging for close monitoring
eval_freq: 250  # Evaluation frequency
no_eval: False
ckpt_freq: 500  # Checkpoint frequency
use_gradient_checkpointing: True  # Reduce memory usage
bf16: True  # Use bfloat16 precision on A100 GPUs
gradient_clipping: 1.0  # Clip gradients to prevent exploding gradients

save_adapters: True  # Save only LoRA adapters

run_dir: "/home/rileywebb/mistral-finetune/output-dir/full_finetune"

wandb:
  project: "suada"
  run_name: "full_finetune"
  key: "your-wandb-api-key"  # Replace with your actual WandB API key
  offline: False
