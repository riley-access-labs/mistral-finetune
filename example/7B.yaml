# data
data:
  instruct_data: "/path/to/cleaned_onlyfans_dataset.jsonl"  # Update with your dataset path
  data: ""  # No pretraining data used
  eval_instruct_data: "/path/to/onlyfans_eval_dataset.jsonl"  # Optionally update if you have a validation set

# model
model_id_or_path: "/path/to/mistral_models/7B_instruct"  # Update with your model path
lora:
  rank: 64

# optim
seq_len: 32768
batch_size: 1  # Batch size limited to 1 due to hardware constraints
gradient_accumulation_steps: 8  # Accumulate gradients over 8 steps to simulate a larger batch size
max_steps: 1000
optim:
  lr: 5e-5  # Adjusted learning rate
  weight_decay: 0.01
  pct_start: 0.05

# other
seed: 42
log_freq: 10
eval_freq: 200
no_eval: False
ckpt_freq: 500

save_adapters: True  # Save only trained LoRA adapters

run_dir: "/path/to/output-dir/onlyfans-finetune"  # Update with your desired output directory

wandb:
  project: "suada"
  run_name: "onlyfans_finetune"
  key: "your_wandb_api_key"
  offline: False
