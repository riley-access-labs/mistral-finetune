# data
data:
  instruct_data: "/home/rileywebb/botly-dataset.jsonl"  # Update with your dataset path
  data: ""  # No pretraining data used
  eval_instruct_data: "/home/rileywebb/botly-dataset-eval.jsonl"  # Optionally update if you have a validation set

# model
model_id_or_path: "/home/rileywebb/mistral_models/7B"  # Update with your model path
lora:
  rank: 64

# optim
seq_len: 32768
batch_size: 1
max_steps: 165
optim:
  lr: 1e-5
  weight_decay: 0.01
  pct_start: 0.05

# other
seed: 0
log_freq: 1
eval_freq: 100
no_eval: False
ckpt_freq: 100

save_adapters: False  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model

run_dir: "/home/rileywebb/mistral-finetune/output-dir/onlyfans-tune"  # Fill

wandb:
  project: "suada" # your wandb project name
  run_name: "onlyfans_tune" # your wandb run name
  key: "35a03d1d2de16b755251b46657e20294b34e1305" # your wandb api key
  offline: False
